{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OlnzXxhzUFMw",
        "7QLQ2N34qHMV",
        "9MAJygPb5mg4",
        "VtWhxTA3Byfn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Acondicionamiento de datos\n",
        "\n",
        "En esta sección se escaliza y se acondicionan los datos para ser ingresados al modelo LSTM. Los datos que este modelo acepta a la entrada es de tres dimensiones (X,Y,Z). Donde X represneta la cantidad de lotes de datos ingresados al modelo. Y, el tamaño del lote, que para este caso es 18, que representa 18 horas. Z, la cantidad de caracteristicas del dataset, para este caso son 9. Por lo anterior los datos que aceptara el modelo son de la forma (X,18,9)"
      ],
      "metadata": {
        "id": "OlnzXxhzUFMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Libreria necesarias para el tratamiento de los datos\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "IFHS-mNgWTGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Carga dataset de datos para entrenamiento\n",
        "\n",
        "df2 = pd.read_csv(\"Dataset_Julio28_Septiembre17_5h.csv\")\n",
        "df2['Date'] = pd.to_datetime(df2['Date'],format = '%Y-%m-%d %H:%M')\n",
        "df2 = df2.set_index('Date')\n",
        "df2.sort_index(inplace=True)\n",
        "df = df2.reset_index()\n",
        "df = df.drop(columns=['Date'])\n",
        "\n",
        "#########################################################################\n",
        "'''\n",
        "Como el dataset tiene secciones donde hacen falta datos de varios días\n",
        "y los datos del sensor de humedad fallaron, se realizó un seccionamiento\n",
        "para eliminar esas inconsistencias y generar bloques de datos del dataset\n",
        "que fueran continuos y sin errores\n",
        "'''\n",
        "\n",
        "dt1 = datetime.strptime(\"2023-07-28 21:32:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "dt2 = datetime.strptime(\"2023-07-31 08:53:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "dt3 = datetime.strptime(\"2023-07-31 20:16:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "dt4 = datetime.strptime(\"2023-08-04 13:17:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "dt5 = datetime.strptime(\"2023-08-04 15:42:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "dt6 = datetime.strptime(\"2023-08-06 22:13:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "dt7 = datetime.strptime(\"2023-08-07 07:52:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "dt8 = datetime.strptime(\"2023-08-20 17:46:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "dt9 = datetime.strptime(\"2023-08-21 09:49:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "dt10 = datetime.strptime(\"2023-08-31 05:28:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "dt11 = datetime.strptime(\"2023-09-02 20:40:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "dt12 = datetime.strptime(\"2023-09-07 22:48:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "dt13 = datetime.strptime(\"2023-09-07 22:59:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "dt14 = datetime.strptime(\"2023-09-12 17:01:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "dt15 = datetime.strptime(\"2023-09-12 17:11:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "dt16 = datetime.strptime(\"2023-09-17 11:34:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "data_rng1 = df2[(df2.index >= dt1 ) & (df2.index <= dt2)]\n",
        "data_rng1 = data_rng1.iloc[::6]\n",
        "data_rng2 = df2[(df2.index >= dt3 ) & (df2.index <= dt4)]\n",
        "data_rng2 = data_rng2.iloc[::6]\n",
        "data_rng3 = df2[(df2.index >= dt5 ) & (df2.index <= dt6)]\n",
        "data_rng3 = data_rng3.iloc[::6]\n",
        "data_rng4 = df2[(df2.index >= dt7 ) & (df2.index <= dt8)]\n",
        "data_rng4 = data_rng4.iloc[::6]\n",
        "data_rng5 = df2[(df2.index >= dt9 ) & (df2.index <= dt10)]\n",
        "data_rng5 = data_rng5.iloc[::6]\n",
        "data_rng6 = df2[(df2.index >= dt11 ) & (df2.index <= dt12)]\n",
        "data_rng6 = data_rng6.iloc[::6]\n",
        "data_rng7 = df2[(df2.index >= dt13 ) & (df2.index <= dt14)]\n",
        "data_rng7 = data_rng7.iloc[::6]\n",
        "data_rng8 = df2[(df2.index >= dt15 ) & (df2.index <= dt16)]\n",
        "data_rng8 = data_rng8.iloc[::6]\n",
        "\n",
        "#########################################################################\n"
      ],
      "metadata": {
        "id": "CeaSYJPWgkWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Carga dataset de datos para prueba\n",
        "\n",
        "df21 = pd.read_csv('/content/Dataset_Septiembre21_Octubre3_10h.csv')\n",
        "df21['Date'] = pd.to_datetime(df21['Date'],format = '%Y-%m-%d %H:%M')\n",
        "df21 = df21.set_index('Date')\n",
        "df21.sort_index(inplace=True)\n",
        "df1 = df21.reset_index()\n",
        "df1 = df1.drop(columns=['Date'])\n",
        "\n",
        "#########################################################################\n",
        "'''\n",
        "Como la frecuencia de los datos es de cada 10 minutos, se cambío a\n",
        "una frecuencia de 1 hora para reducción del tamaño final del modelo\n",
        "al igual que los datos de entrenamiento\n",
        "'''\n",
        "df21 = df21.iloc[::6]\n",
        "df1 = df1.iloc[::6]\n",
        "#########################################################################"
      ],
      "metadata": {
        "id": "U4_HKn5yW44f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensamble para generar dataset de 10 y 15 horas\n",
        "'''\n",
        "Si se ha cargado el archivo de 5 horas \"Dataset_Julio28_Septiembre17_5h.csv\"\n",
        "y se quiere hacer el entrenamiento para predecir 10 o 15 horas se debe ejecutar\n",
        "esta celda, de lo contrario omita ésta\n",
        "'''\n",
        "\n",
        "CORRIMIENTO = 5  #Para 10 horas = 5   para 15 horas = 10\n",
        "\n",
        "data_rng1_10h = data_rng1.drop(columns=['S_temperature','S_humidity', 'S_uv',\n",
        "                                        'S_soilMoisture','S_anRain'])\n",
        "data_rng1_10h = data_rng1_10h.iloc[CORRIMIENTO:]\n",
        "data_rng1_10h = data_rng1_10h.reset_index()\n",
        "data_rng1_10h = data_rng1_10h.drop(columns=['Date'])\n",
        "data_rng1_sensors = data_rng1.drop(columns=['Temperature_5h','Humidity_5h',\n",
        "                                            'Clouds_5h','PoP_5h'])\n",
        "data_rng1_sensors = data_rng1_sensors.iloc[:-CORRIMIENTO]\n",
        "data_rng1_sensors = data_rng1_sensors.reset_index()\n",
        "data_rng1_sensors = data_rng1_sensors.drop(columns=['Date'])\n",
        "data_rng1= pd.concat([data_rng1_sensors, data_rng1_10h], axis=1)\n",
        "\n",
        "\n",
        "data_rng2_10h = data_rng2.drop(columns=['S_temperature','S_humidity', 'S_uv',\n",
        "                                        'S_soilMoisture','S_anRain'])\n",
        "data_rng2_10h = data_rng2_10h.iloc[CORRIMIENTO:]\n",
        "data_rng2_10h = data_rng2_10h.reset_index()\n",
        "data_rng2_10h = data_rng2_10h.drop(columns=['Date'])\n",
        "data_rng2_sensors = data_rng2.drop(columns=['Temperature_5h','Humidity_5h',\n",
        "                                            'Clouds_5h','PoP_5h'])\n",
        "data_rng2_sensors = data_rng2_sensors.iloc[:-CORRIMIENTO]\n",
        "data_rng2_sensors = data_rng2_sensors.reset_index()\n",
        "data_rng2_sensors = data_rng2_sensors.drop(columns=['Date'])\n",
        "data_rng2= pd.concat([data_rng2_sensors, data_rng2_10h], axis=1)\n",
        "\n",
        "\n",
        "data_rng3_10h = data_rng3.drop(columns=['S_temperature','S_humidity', 'S_uv',\n",
        "                                          'S_soilMoisture','S_anRain'])\n",
        "data_rng3_10h = data_rng3_10h.iloc[CORRIMIENTO:]\n",
        "data_rng3_10h = data_rng3_10h.reset_index()\n",
        "data_rng3_10h = data_rng3_10h.drop(columns=['Date'])\n",
        "data_rng3_sensors = data_rng3.drop(columns=['Temperature_5h','Humidity_5h',\n",
        "                                            'Clouds_5h','PoP_5h'])\n",
        "data_rng3_sensors = data_rng3_sensors.iloc[:-CORRIMIENTO]\n",
        "data_rng3_sensors = data_rng3_sensors.reset_index()\n",
        "data_rng3_sensors = data_rng3_sensors.drop(columns=['Date'])\n",
        "\n",
        "data_rng3= pd.concat([data_rng3_sensors, data_rng3_10h], axis=1)\n",
        "\n",
        "data_rng4_10h = data_rng4.drop(columns=['S_temperature','S_humidity', 'S_uv',\n",
        "                                        'S_soilMoisture','S_anRain'])\n",
        "data_rng4_10h = data_rng4_10h.iloc[CORRIMIENTO:]\n",
        "data_rng4_10h = data_rng4_10h.reset_index()\n",
        "data_rng4_10h = data_rng4_10h.drop(columns=['Date'])\n",
        "data_rng4_sensors = data_rng4.drop(columns=['Temperature_5h','Humidity_5h',\n",
        "                                            'Clouds_5h','PoP_5h'])\n",
        "data_rng4_sensors = data_rng4_sensors.iloc[:-CORRIMIENTO]\n",
        "data_rng4_sensors = data_rng4_sensors.reset_index()\n",
        "data_rng4_sensors = data_rng4_sensors.drop(columns=['Date'])\n",
        "\n",
        "data_rng4= pd.concat([data_rng4_sensors, data_rng4_10h], axis=1)\n",
        "\n",
        "data_rng5_10h = data_rng5.drop(columns=['S_temperature','S_humidity', 'S_uv',\n",
        "                                        'S_soilMoisture','S_anRain'])\n",
        "data_rng5_10h = data_rng5_10h.iloc[CORRIMIENTO:]\n",
        "data_rng5_10h = data_rng5_10h.reset_index()\n",
        "data_rng5_10h = data_rng5_10h.drop(columns=['Date'])\n",
        "data_rng5_sensors = data_rng5.drop(columns=['Temperature_5h','Humidity_5h',\n",
        "                                            'Clouds_5h','PoP_5h'])\n",
        "data_rng5_sensors = data_rng5_sensors.iloc[:-CORRIMIENTO]\n",
        "data_rng5_sensors = data_rng5_sensors.reset_index()\n",
        "data_rng5_sensors = data_rng5_sensors.drop(columns=['Date'])\n",
        "data_rng5= pd.concat([data_rng5_sensors, data_rng5_10h], axis=1)\n",
        "\n",
        "data_rng6_10h = data_rng6.drop(columns=['S_temperature','S_humidity', 'S_uv',\n",
        "                                        'S_soilMoisture','S_anRain'])\n",
        "data_rng6_10h = data_rng6_10h.iloc[CORRIMIENTO:]\n",
        "data_rng6_10h = data_rng6_10h.reset_index()\n",
        "data_rng6_10h = data_rng6_10h.drop(columns=['Date'])\n",
        "data_rng6_sensors = data_rng6.drop(columns=['Temperature_5h','Humidity_5h',\n",
        "                                            'Clouds_5h','PoP_5h'])\n",
        "data_rng6_sensors = data_rng6_sensors.iloc[:-CORRIMIENTO]\n",
        "data_rng6_sensors = data_rng6_sensors.reset_index()\n",
        "data_rng6_sensors = data_rng6_sensors.drop(columns=['Date'])\n",
        "data_rng6= pd.concat([data_rng6_sensors, data_rng6_10h], axis=1)\n",
        "\n",
        "data_rng7_10h = data_rng7.drop(columns=['S_temperature','S_humidity', 'S_uv',\n",
        "                                        'S_soilMoisture','S_anRain'])\n",
        "data_rng7_10h = data_rng7_10h.iloc[CORRIMIENTO:]\n",
        "data_rng7_10h = data_rng7_10h.reset_index()\n",
        "data_rng7_10h = data_rng7_10h.drop(columns=['Date'])\n",
        "data_rng7_sensors = data_rng7.drop(columns=['Temperature_5h','Humidity_5h',\n",
        "                                            'Clouds_5h','PoP_5h'])\n",
        "data_rng7_sensors = data_rng7_sensors.iloc[:-CORRIMIENTO]\n",
        "data_rng7_sensors = data_rng7_sensors.reset_index()\n",
        "data_rng7_sensors = data_rng7_sensors.drop(columns=['Date'])\n",
        "data_rng7= pd.concat([data_rng7_sensors, data_rng7_10h], axis=1)\n",
        "\n",
        "data_rng8_10h = data_rng8.drop(columns=['S_temperature','S_humidity', 'S_uv',\n",
        "                                        'S_soilMoisture','S_anRain'])\n",
        "data_rng8_10h = data_rng8_10h.iloc[CORRIMIENTO:]\n",
        "data_rng8_10h = data_rng8_10h.reset_index()\n",
        "data_rng8_10h = data_rng8_10h.drop(columns=['Date'])\n",
        "data_rng8_sensors = data_rng8.drop(columns=['Temperature_5h','Humidity_5h',\n",
        "                                            'Clouds_5h','PoP_5h'])\n",
        "data_rng8_sensors = data_rng8_sensors.iloc[:-CORRIMIENTO]\n",
        "data_rng8_sensors = data_rng8_sensors.reset_index()\n",
        "data_rng8_sensors = data_rng8_sensors.drop(columns=['Date'])\n",
        "data_rng8= pd.concat([data_rng8_sensors, data_rng8_10h], axis=1)"
      ],
      "metadata": {
        "id": "qUNsKUJsguKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparación para dataset hasta de 5 horas\n",
        "\n",
        "'''\n",
        "Si se ejecuto la celda anterior \"Ensamble para generar dataset de 10 y 15 horas\"\n",
        "no ejecute esta.\n",
        "Esta celda organiza los datos para los datasets para predecir hasta 5 horas\n",
        "'''\n",
        "\n",
        "data_rng1 = data_rng1.reset_index()\n",
        "data_rng1 = data_rng1.drop(columns=['Date'])\n",
        "data_rng2 = data_rng2.reset_index()\n",
        "data_rng2 = data_rng2.drop(columns=['Date'])\n",
        "data_rng3 = data_rng3.reset_index()\n",
        "data_rng3 = data_rng3.drop(columns=['Date'])\n",
        "data_rng4 = data_rng4.reset_index()\n",
        "data_rng4 = data_rng4.drop(columns=['Date'])\n",
        "data_rng5 = data_rng5.reset_index()\n",
        "data_rng5 = data_rng5.drop(columns=['Date'])\n",
        "data_rng6 = data_rng6.reset_index()\n",
        "data_rng6 = data_rng6.drop(columns=['Date'])\n",
        "data_rng7 = data_rng7.reset_index()\n",
        "data_rng7 = data_rng7.drop(columns=['Date'])\n",
        "data_rng8 = data_rng8.reset_index()\n",
        "data_rng8 = data_rng8.drop(columns=['Date'])"
      ],
      "metadata": {
        "id": "aNNm8buAfgN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Funciones para crear dataset supervisado y para escalizar los datos\n",
        "\n",
        "'''\n",
        "Esta función genera los dataset para entrenamiento, validación y prueba a\n",
        "partir del dataset de entrenamiento\n",
        "'''\n",
        "def crear_dataset_supervisado(array, input_length, output_length,corrimiento):\n",
        "    input_length2= corrimiento + input_length\n",
        "    # Arreglo para datos de entrada y salida generados\n",
        "    X, Y = [], []\n",
        "    shape = array.shape\n",
        "    fils, cols = array.shape\n",
        "    for i in range(fils-input_length-output_length):\n",
        "        if(i+input_length2+output_length)<=fils:\n",
        "            X.append(array[i:i+input_length,0:cols])\n",
        "            # Salida (el índice 3 corresponde a la columna con la variable a predecir)\n",
        "            Y.append(array[i+input_length2:i+input_length2+output_length,3].reshape(output_length,1))\n",
        "\n",
        "    # Convertir listas a arreglos de NumPy\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "'''\n",
        "Esta función escala los datos para cada característica del dataset\n",
        "guarda la lista de escalador en el archivo \"lista_scalers.pkl\" y retorna\n",
        "los valores escalados en un diccionario y el escalador de la variable de interes\n",
        "'''\n",
        "\n",
        "def escalar_dataset(data_input, col_ref):\n",
        "    col_ref = df.columns.get_loc(col_ref)\n",
        "    NFEATS = data_input['x_tr'].shape[2]\n",
        "    # Generar listado de escaladores para las 9 característcas.\n",
        "    scalers = [MinMaxScaler(feature_range=(-1,1)) for i in range(NFEATS)]\n",
        "    # Arreglos para los datasets escalados\n",
        "    x_tr_s = np.zeros(data_input['x_tr'].shape)\n",
        "    x_vl_s = np.zeros(data_input['x_vl'].shape)\n",
        "    x_ts_s = np.zeros(data_input['x_ts'].shape)\n",
        "    x_ts_sN = np.zeros(data_input['x_tsN'].shape)\n",
        "    y_tr_s = np.zeros(data_input['y_tr'].shape)\n",
        "    y_vl_s = np.zeros(data_input['y_vl'].shape)\n",
        "    y_ts_s = np.zeros(data_input['y_ts'].shape)\n",
        "    y_ts_sN = np.zeros(data_input['y_tsN'].shape)\n",
        "\n",
        "    for i in range(NFEATS):\n",
        "        x_tr_s[:, :, i] = scalers[i].fit_transform(x_tr[:, :, i].reshape(-1, 1)).reshape(x_tr.shape[0], x_tr.shape[1])\n",
        "        x_ts_sN[:, :, i] = scalers[i].fit_transform(x_tsN[:, :, i].reshape(-1, 1)).reshape(x_tsN.shape[0], x_tsN.shape[1])\n",
        "        x_vl_s[:, :, i] = scalers[i].fit_transform(x_vl[:, :, i].reshape(-1, 1)).reshape(x_vl.shape[0], x_vl.shape[1])\n",
        "        x_ts_s[:, :, i] = scalers[i].fit_transform(x_ts[:, :, i].reshape(-1, 1)).reshape(x_ts.shape[0], x_ts.shape[1])\n",
        "\n",
        "    y_tr_s[:, :, 0] = scalers[col_ref].fit_transform(y_tr[:, :, 0].reshape(-1, 1)).reshape(y_tr.shape[0], y_tr.shape[1])\n",
        "    y_ts_sN[:, :, 0] = scalers[col_ref].fit_transform(y_tsN[:, :, 0].reshape(-1, 1)).reshape(y_tsN.shape[0], y_tsN.shape[1])\n",
        "    y_vl_s[:, :, 0] = scalers[col_ref].fit_transform(y_vl[:, :, 0].reshape(-1, 1)).reshape(x_vl.shape[0], y_vl.shape[1])\n",
        "    y_ts_s[:, :, 0] = scalers[col_ref].fit_transform(y_ts[:, :, 0].reshape(-1, 1)).reshape(y_ts.shape[0], y_ts.shape[1])\n",
        "\n",
        "    # Diccionario de salida\n",
        "    data_scaled = {\n",
        "        'x_tr_s': x_tr_s, 'y_tr_s': y_tr_s,\n",
        "        'x_vl_s': x_vl_s, 'y_vl_s': y_vl_s,\n",
        "        'x_ts_s': x_ts_s, 'y_ts_s': y_ts_s,\n",
        "        'x_ts_sN': x_ts_sN, 'y_ts_sN': y_ts_sN,\n",
        "    }\n",
        "    #Guarda la lista de escaladores en una lista\n",
        "    with open('lista_scalers.pkl', 'wb') as file:\n",
        "        pickle.dump(scalers, file)\n",
        "\n",
        "    return data_scaled, scalers[col_ref]"
      ],
      "metadata": {
        "id": "3tq_5yFAg2Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Genera los datos de entrenamiento, validación y prueba\n",
        "\n",
        "# Crear los datasets de entrenamiento, prueba y validación y verificar sus tamaños\n",
        "INPUT_LENGTH = 18    # Hiperparámetro ajustado para 18 horas de entrada al modelo LSTM\n",
        "OUTPUT_LENGTH = 1    # Salida del modelo LSTM\n",
        "corrimiento = 9      # saltos de 1h  1h=0 2h=1 3h=2 4h=3 5h=4 10h=9 15h=14\n",
        "\n",
        "##########Para datos de prueba con el datasaet para preubas###############\n",
        "x_tsN, y_tsN = crear_dataset_supervisado(df1.values, INPUT_LENGTH, OUTPUT_LENGTH,corrimiento)\n",
        "############################################################\n",
        "\n",
        "x_dt1, y_dt1 = crear_dataset_supervisado(data_rng1.values, INPUT_LENGTH, OUTPUT_LENGTH,corrimiento)\n",
        "x_dt2, y_dt2 = crear_dataset_supervisado(data_rng2.values, INPUT_LENGTH, OUTPUT_LENGTH,corrimiento)\n",
        "x_dt3, y_dt3 = crear_dataset_supervisado(data_rng3.values, INPUT_LENGTH, OUTPUT_LENGTH,corrimiento)\n",
        "x_dt4, y_dt4 = crear_dataset_supervisado(data_rng4.values, INPUT_LENGTH, OUTPUT_LENGTH,corrimiento)\n",
        "x_dt5, y_dt5 = crear_dataset_supervisado(data_rng5.values, INPUT_LENGTH, OUTPUT_LENGTH,corrimiento)\n",
        "x_dt6, y_dt6 = crear_dataset_supervisado(data_rng6.values, INPUT_LENGTH, OUTPUT_LENGTH,corrimiento)\n",
        "x_dt7, y_dt7 = crear_dataset_supervisado(data_rng7.values, INPUT_LENGTH, OUTPUT_LENGTH,corrimiento)\n",
        "x_dt8, y_dt8 = crear_dataset_supervisado(data_rng8.values, INPUT_LENGTH, OUTPUT_LENGTH,corrimiento)\n",
        "\n",
        "x_data = np.concatenate((x_dt1, x_dt2, x_dt3, x_dt4, x_dt5, x_dt6, x_dt7, x_dt8), axis=0)\n",
        "y_data = np.concatenate((y_dt1, y_dt2, y_dt3, y_dt4, y_dt5, y_dt6, y_dt7, y_dt8), axis=0)\n",
        "\n",
        "x_tr, x_validation, y_tr, y_validation = train_test_split(\n",
        "    x_data, y_data, test_size=0.2, random_state=42)\n",
        "x_vl, x_ts, y_vl, y_ts = train_test_split(\n",
        "    x_validation, y_validation, test_size=0.5, random_state=42)\n",
        "\n",
        "print('Tamaños entrada (BATCHES x INPUT_LENGTH x FEATURES) y de salida (BATCHES x OUTPUT_LENGTH x FEATURES)')\n",
        "print(f'Set de entrenamiento - x_tr: {x_tr.shape}, y_tr: {y_tr.shape}')\n",
        "print(f'Set de validación - x_vl: {x_vl.shape}, y_vl: {y_vl.shape}')\n",
        "print(f'Set de prueba - x_ts: {x_ts.shape}, y_ts: {y_ts.shape}')"
      ],
      "metadata": {
        "id": "dSzo88yzg8_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Escalamiento de los datos\n",
        "\n",
        "# Crear diccionario de entrada\n",
        "data_in = {\n",
        "    'x_tr': x_tr, 'y_tr': y_tr,\n",
        "    'x_vl': x_vl, 'y_vl': y_vl,\n",
        "    'x_ts': x_ts, 'y_ts': y_ts,\n",
        "    'x_tsN':x_tsN,'y_tsN': y_tsN,\n",
        "}\n",
        "\n",
        "# Escala los valores espacificando la variable de interes\n",
        "data_s, scaler = escalar_dataset(data_in, col_ref = 'S_soilMoisture' )\n",
        "\n",
        "# Extraer subsets escalados\n",
        "x_tr_s, y_tr_s = data_s['x_tr_s'], data_s['y_tr_s']\n",
        "x_vl_s, y_vl_s = data_s['x_vl_s'], data_s['y_vl_s']\n",
        "x_ts_s, y_ts_s = data_s['x_ts_s'], data_s['y_ts_s']\n",
        "x_ts_sN, y_ts_sN = data_s['x_ts_sN'], data_s['y_ts_sN']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,4))\n",
        "for i in range(9):\n",
        "    ax.violinplot(dataset=x_tr_s[:,:,i].flatten(), positions=[i])\n",
        "    ax.violinplot(dataset=x_vl_s[:,:,i].flatten(), positions=[i])\n",
        "    ax.violinplot(dataset=x_ts_s[:,:,i].flatten(), positions=[i])\n",
        "\n",
        "ax.set_xticks(list(range(9)))\n",
        "ax.set_xticklabels(df.keys(), rotation=45)\n",
        "ax.autoscale();\n",
        "\n",
        "#Guarda escalador de salida\n",
        "with open('scaler_y.pkl', 'wb') as file:\n",
        "        pickle.dump(scaler, file)"
      ],
      "metadata": {
        "id": "UfWzsPDVhCAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cambia el formato de 64 a 32 bits\n",
        "\n",
        "x_tr = x_tr.astype('float32')\n",
        "x_vl = x_vl.astype('float32')\n",
        "x_ts = x_ts.astype('float32')\n",
        "x_tsN= x_tsN.astype('float32')\n",
        "\n",
        "y_tr = y_tr.astype('float32')\n",
        "y_vl = y_vl.astype('float32')\n",
        "y_ts = y_ts.astype('float32')\n",
        "y_tsN= y_tsN.astype('float32')\n",
        "\n",
        "x_tr_s = x_tr_s.astype('float32')\n",
        "x_vl_s = x_vl_s.astype('float32')\n",
        "x_ts_s = x_ts_s.astype('float32')\n",
        "x_ts_sN = x_ts_sN.astype('float32')\n",
        "\n",
        "y_tr_s = y_tr_s.astype('float32')\n",
        "y_vl_s = y_vl_s.astype('float32')\n",
        "y_ts_s = y_ts_s.astype('float32')\n",
        "y_ts_sN= y_ts_sN.astype('float32')"
      ],
      "metadata": {
        "id": "Ssp3i6ilnsSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entrenamiento del modelo LSTM\n",
        "\n",
        "Esta sección ejecuta el entrenamiento para el modelo LSTM"
      ],
      "metadata": {
        "id": "7QLQ2N34qHMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "metadata": {
        "id": "Gls6sEh0C8Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creacíon del modelo\n",
        "\n",
        "# Ajuste de parámetros para reproducibilidad del entrenamiento\n",
        "tf.random.set_seed(123)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "# El modelo\n",
        "N_UNITS = 50 # Tamaño del estado oculto (h) y de la celdad de memoria (c) (128) 50\n",
        "INPUT_SHAPE = (x_tr_s.shape[1], x_tr_s.shape[2]) # 24 (horas) x 13 (features)\n",
        "modelo = Sequential()\n",
        "modelo.add(LSTM(N_UNITS, input_shape=INPUT_SHAPE))\n",
        "modelo.add(Dense(OUTPUT_LENGTH, activation='linear')) # activation = 'linear' pues se quiere pronosticar (regresión)\n",
        "\n",
        "# Pérdida: se usará el RMSE (root mean squared error) para el entrenamiento\n",
        "# pues permite tener errores en las mismas unidades de la humedad\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    rmse = tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred-y_true)))\n",
        "    return rmse\n",
        "\n",
        "# Compilación\n",
        "optimizador = RMSprop(learning_rate=15e-5) # 5e-4\n",
        "modelo.compile(\n",
        "    optimizer = optimizador,\n",
        "    loss = root_mean_squared_error,\n",
        ")"
      ],
      "metadata": {
        "id": "cgycoVzWhHV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenamiento del modelo LSTM\n",
        "\n",
        "EPOCHS = 250\n",
        "BATCH_SIZE = 256\n",
        "historia = modelo.fit(\n",
        "    x = x_tr_s,\n",
        "    y = y_tr_s,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    epochs = EPOCHS,\n",
        "    validation_data = (x_vl_s, y_vl_s),\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "id": "NJDL7bgpxD7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Grafica curvas de entrenamiento y validación\n",
        "\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(historia.history['loss'],label='RMSE train')\n",
        "plt.plot(historia.history['val_loss'],label='RMSE val')\n",
        "plt.xlabel('Iteración')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "plt.title(\"15 horas\")\n",
        "plt.show()\n",
        "\n",
        "# Cálculo de rmses para train, val y test\n",
        "rmse_tr = modelo.evaluate(x=x_tr_s, y=y_tr_s, verbose=0)\n",
        "rmse_vl = modelo.evaluate(x=x_vl_s, y=y_vl_s, verbose=0)\n",
        "rmse_ts = modelo.evaluate(x=x_ts_s, y=y_ts_s, verbose=0)\n",
        "\n",
        "#print('Comparativo desempeños:', i)\n",
        "print(f'  RMSE train:\\t  {rmse_tr:.3f}')\n",
        "print(f'  RMSE val:\\t {rmse_vl:.3f}')\n",
        "print(f'  RMSE test:\\t {rmse_ts:.3f}')"
      ],
      "metadata": {
        "id": "175jv4drw_eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencias con el modelo LSTM\n",
        "\n",
        "Esta sección realiza inferencias con el modelo LSTM previamente entrenado"
      ],
      "metadata": {
        "id": "9MAJygPb5mg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Función para inferencias\n",
        "\n",
        "'''\n",
        "Esta función retorna el valor estimado de la humedad del suelo\n",
        "escalado a sus valores originales\n",
        "'''\n",
        "def predecir(x, model, scaler):\n",
        "    y_pred_s = model.predict(x,verbose=0)\n",
        "\n",
        "    # Llevar la predicción a la escala original\n",
        "    y_pred = scaler.inverse_transform(y_pred_s)\n",
        "\n",
        "    return y_pred.flatten()"
      ],
      "metadata": {
        "id": "QOWbqlhVrdPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Estimación de la humedad del suelo\n",
        "\n",
        "expected = predecir(x_ts_sN, modelo, scaler)\n",
        "real = y_tsN.flatten()\n",
        "fig1 = plt.figure(\"Predicciones de humedad del suelo (18 entradas(18h))\")\n",
        "fig1.suptitle(\"Predicciones de humedad del suelo (18 entradas(18h))\")\n",
        "fig1.subplots_adjust(hspace=0.5, wspace=0.5)\n",
        "\n",
        "ax = fig1.add_subplot(1, 1, 1)\n",
        "ax.plot(real)\n",
        "ax.plot(expected)\n",
        "ax.plot(y_tsN.flatten())\n",
        "ax.set_xlabel(\"Muestras\")\n",
        "ax.set_ylabel(\"% Humedad del suelo\")\n",
        "ax.set_title(\"10 Horas\")\n",
        "ax.grid(color='gray', linestyle='dashed', linewidth=1, alpha=0.4)\n",
        "ax.axhline(0, color='black', linewidth=0.5)\n",
        "fig1.legend([\"real\",\"expected\"], loc =\"lower right\")"
      ],
      "metadata": {
        "id": "Py4RKiQt-V1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversion a TinyML\n",
        "\n",
        "Esta sección convierte el modelo de ML a tinyML para usarse en un microcontrolador"
      ],
      "metadata": {
        "id": "VtWhxTA3Byfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Conversión del modelo a Tinyml\n",
        "\n",
        "run_model = tf.function(lambda x: modelo(x))\n",
        "# This is important, let's fix the input size.\n",
        "BATCH_SIZE = 1\n",
        "STEPS = 18\n",
        "INPUT_SIZE = 9\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], modelo.inputs[0].dtype))\n",
        "\n",
        "# model directory.\n",
        "MODEL_DIR = \"keras_lstm\"\n",
        "modelo.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n",
        "tflite_model = converter.convert()"
      ],
      "metadata": {
        "id": "Iy0I5jSCiocH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guarda en archivo el modelo convertido\n",
        "open(\"LSTM_model.tflite\", \"wb\").write(tflite_model)"
      ],
      "metadata": {
        "id": "L3d3YPE8Hfu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Corre el modelo de TensorFlow Lite\n",
        "\n",
        "'''\n",
        "Ejecuta el modelo de TensorGlow Lite y hace 10 comparaciones conel modelo\n",
        "de TensorFLow para verificar la conversión\n",
        "'''\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "TEST_CASES=10\n",
        "\n",
        "for i in range(TEST_CASES):\n",
        "  expected = modelo.predict(x_ts_sN[i:i+1])\n",
        "  interpreter.set_tensor(input_details[0][\"index\"], x_ts_sN[i:i+1, :, :])\n",
        "  interpreter.invoke()\n",
        "  result = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "  np.testing.assert_almost_equal(expected, result, decimal=5)\n",
        "  print(\"El resultado de TensorFlow concuerda con el de TensorFlow Lite\")\n",
        "  interpreter.reset_all_variables()"
      ],
      "metadata": {
        "id": "C2xv5AUiH1Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evalua el tamño del archivo del modelo convertido\n",
        "\n",
        "import os\n",
        "basic_model_size = os.path.getsize(\"LSTM_model.tflite\")\n",
        "print(\"El tamaño del modelo es de %d bytes\" % basic_model_size)"
      ],
      "metadata": {
        "id": "XN9fBF5GIiqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instala la herramienta xxd\n",
        "'''\n",
        "Esta herramienta permite convertir el modelo a un arreglo de valores\n",
        "hexadecimal\n",
        "'''\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install xxd"
      ],
      "metadata": {
        "id": "k9-eDQgnKX05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convierte y guarda el modelo como un archivo en C\n",
        "\n",
        "!xxd -i LSTM_model.tflite > LSTM_model.cc\n",
        "!cat LSTM_model.cc"
      ],
      "metadata": {
        "id": "QicWUUjdMEQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}